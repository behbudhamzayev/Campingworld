# Campingworld

'''
https://rv.campingworld.com/robots.txt
Webpage disallows scraping for some information in links like "Rvdetails" But as a student and noncommercial practice proceeding, 
yet could be stumbled on some error further because of  User-agent: */mod-security.

First impressions of webpage:

* Design and structuring have done poorly, as multiple tags with the same class/id. Which is not only discomfort to scrape but also style site.
* search engine is not consistent
* website is pretty slow and sometimes can not show all the details or shows a blank page

The obstacles were mainly about website response, even for casually surfing, which led to irritation and squandering of time for nothing, 
and website blocking crawlers and crashing. There were times when it totally intercepted my API. A combination of such issues, 
whether the structure of the website and design, response time, and strictly blocked crawler, made the particular task a bit laborious 
but quite intriguing and educational. There are many possible ways to perform given task points, even in Scrapy itself, 
yet my chosen method of structuring scraping is far easier to catch and quite easily detect problems.

For more detail description please check rvmoto.py file.


'''
